# -*- coding: utf-8 -*-
"""pwmspark.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1n2Tp4ZYUMgdgwqbyFhrcz19_Go0kkLn5
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, sum, when, length,udf
from pyspark.sql import functions as F
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
import nltk
import string
!pip install nltk
import nltk
nltk.download('averaged_perceptron_tagger_eng')
nltk.download('wordnet')
nltk.download('omw-1.4')
import spacy  # Ajout de spaCy pour le NER
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from sklearn import metrics
from sklearn.metrics import roc_curve, auc
from nltk.stem.porter import PorterStemmer
from nltk.corpus import stopwords
from nltk import word_tokenize
from nltk.stem import WordNetLemmatizer
from nltk import pos_tag
from pyspark.sql.types import *
from pyspark.sql.types import DoubleType
from pyspark.ml.feature import Tokenizer
from pyspark.ml.feature import StopWordsRemover
from pyspark.ml.feature import HashingTF, IDF
from pyspark.ml.classification import LogisticRegression, NaiveBayes, GBTClassifier
from pyspark.mllib.classification import LogisticRegressionWithLBFGS, LogisticRegressionModel,LogisticRegressionWithSGD
from pyspark.mllib.regression import LabeledPoint
from pyspark.ml import Pipeline
from pyspark.ml.tuning import CrossValidator, ParamGridBuilder
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
from pyspark.ml.evaluation import RegressionEvaluator, BinaryClassificationEvaluator
from pyspark.ml.classification import DecisionTreeClassifier
from pyspark.ml.feature import StringIndexer, VectorIndexer
from pyspark.ml.feature import NGram
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD
from sklearn.cluster import KMeans
from sklearn.preprocessing import MinMaxScaler
from sklearn.manifold import TSNE
import matplotlib.cm as cm
import pandas as pd
from pyspark.ml.recommendation import ALS

# Charger le modèle spaCy pour le NER (assurez-vous que le modèle 'en_core_web_sm' est installé)
nlp = spacy.load("en_core_web_sm")

my_spark=SparkSession.builder.appName("my_spark").getOrCreate()
print(my_spark)

chemin_local = "/content/dataset_projet_web_mining.csv"  #
df = my_spark.read.option("header", "true").csv(chemin_local)
df.show()

"""# Recommendation ALS"""

electro_ratings = df.select(
    col("id").alias("reviewerID"),
    col("asins").alias("ElectroID"),
    col("`reviews.rating`").alias("rating")
).dropna()

user_indexer = StringIndexer(inputCol="reviewerID", outputCol="userIndex")
electro_indexer = StringIndexer(inputCol="ElectroID", outputCol="ElectroIndex")
electro_ratings = user_indexer.fit(electro_ratings).transform(electro_ratings)
electro_ratings = electro_indexer.fit(electro_ratings).transform(electro_ratings)

electro_ratings = electro_ratings.select(
    col("userIndex").cast("int").alias("reviewerID"),
    col("ElectroIndex").cast("int").alias("ElectroID"),
    col("rating").cast("float")
)
electro_ratings.show()

# Initialize ALS with parameters
als = ALS(userCol="reviewerID",
          itemCol="ElectroID",
          ratingCol="rating",
          nonnegative=True,
          coldStartStrategy="drop",
          implicitPrefs=False)

param_grid = ParamGridBuilder() \
                  .addGrid(als.rank, [5, 20]) \
                  .addGrid(als.maxIter, [5]) \
                  .addGrid(als.regParam, [0.01, 0.05, 1]) \
                  .build()

# Define evaluator
reg_eval = RegressionEvaluator(metricName = "rmse",
                               predictionCol = "prediction",
                               labelCol = "rating")
reg_eval_r2 = RegressionEvaluator(metricName="r2", predictionCol="prediction", labelCol="rating")


print(f"Num models to be tested: {len(param_grid)}")

#Creating the crossValidation
cv = CrossValidator(estimator = als,
                    estimatorParamMaps= param_grid,
                    evaluator = reg_eval,
                    numFolds = 5)

# Split data into 80% train, 20% test
training_data, test_data = electro_ratings.randomSplit([0.8, 0.2], seed = 0)
# Remplacer les NaN par une valeur, par exemple 0
training_data = training_data.fillna({'rating': 0})

# Training model
model = cv.fit(training_data)

# Get best model
best_model = model.bestModel

print(type(best_model))

print("\n**Best Model**")
print("  Rank:", best_model.rank)
print("  MaxIter:", best_model._java_obj.parent().getMaxIter())
print("  RegParam:", best_model._java_obj.parent().getRegParam())

# Predict ratings using trained model
predictions = best_model.transform(test_data)
predictions.show(5)

predictions_clean = predictions.filter(predictions['prediction'].isNotNull())
predictions_clean = predictions_clean.filter(predictions_clean['rating'].isNotNull())

# Evaluate the "test_predictions" dataframe
print("RMSE:", reg_eval.evaluate(predictions_clean))
print("R²:", reg_eval_r2.evaluate(predictions_clean))

userRecs = best_model.recommendForAllUsers(5)
userRecs.show(5, truncate = False)

userRecs_pandas = userRecs.toPandas()
userRecs_pandas.head()

def get_user_recommended_videos(recs_df, userId):
  try:
    recommendations = recs_df[recs_df["reviewerID"] == userId]["recommendations"]
    for video in recommendations[0]:
      print(f"VideoID: \n{video[0]}\nPredicted Rating: {video[1]}\n")
  except:
    print("That userId does not exist in the dataset.  Try another.")

get_user_recommended_videos(userRecs_pandas, 1)

# Fonction de détection des entités nommées avec spaCy
def ner_udf(text):
    if text is not None:
        doc = nlp(text)
        # Extraction des entités nommées (noms de personnes, lieux, organisations, etc.)
        entities = [(ent.text, ent.label_) for ent in doc.ents]
        return str(entities)
    else:
        return None

# Créer une UDF PySpark pour appliquer la fonction de NER
udf_ner = udf(ner_udf, StringType())

# Appliquer la fonction NER à chaque avis dans le DataFrame PySpark
df = df.withColumn("named_entities", udf_ner(col("`reviews.text`")))

# Vérifier le résultat
df.select("`reviews.text`", "named_entities").show(10)

df.select([
    sum(when(col(f"`{c}`").isNull() | (col(f"`{c}`") == ""), 1).otherwise(0)).alias(c)
    for c in df.columns
]).show()

df = df.drop("reviews.id", "reviews.userCity", "reviews.userProvince")

print(df.columns)

df.show()

# Fonction sentiment dans PySpark
def sentiment_udf(rating):
    if rating < 3:
        return "Negative"
    elif rating > 3:
        return "Positive"
    else:
        return "Neutral"

# Créer la colonne 'Sentiment' avec PySpark
df = df.withColumn("Sentiment",
                   F.when(F.col("`reviews.rating`") < 3, "Negative")
                    .when(F.col("`reviews.rating`") > 3, "Positive")
                    .otherwise("Neutral"))

# Afficher les 10 premières lignes pour vérifier
df.select("`reviews.rating`", "Sentiment").show(10)

df = df.withColumn("length", length("`reviews.text`"))

# Trouver la longueur maximale
max_length = df.agg({"length": "max"}).collect()[0][0]

# Récupérer l'avis le plus long
longest_review = df.filter(df["length"] == max_length).select("`reviews.text`").collect()[0][0]
print(longest_review)

# Trouver la longueur minimale
min_length = df.agg({"length": "min"}).collect()[0][0]

# Récupérer l'avis avec la longueur minimale
shortest_review = df.filter(df["length"] == min_length).select("`reviews.text`").collect()[0][0]
print(shortest_review)

# Convertir le DataFrame PySpark en pandas DataFrame
pandas_df = df.toPandas()

# Tracer le countplot
plt.figure(figsize=(8, 6))
sns.countplot(x='reviews.rating', data=pandas_df, order=[0, 1, 2, 3, 4, 5])
plt.title("Distribution des évaluations des avis (0 à 5)")
plt.xlabel("Note des avis")
plt.ylabel("Nombre d'avis")
plt.show()

# créer le graphique
plt.figure(figsize=(5,3))
sns.countplot(x="Sentiment", data=pandas_df, palette=["#f8de7e", "#32fa53", "#fa3232"])
plt.title("Rating Count")
plt.show()

fig = plt.figure(figsize=(16,10))
ax1 = plt.subplot(211)
ax2 = plt.subplot(212, sharex = ax1)
pandas_df["asins"].value_counts().plot(kind="bar", ax=ax1, title="ASIN Frequency")
np.log10(pandas_df["asins"].value_counts()).plot(kind="bar", ax=ax2, title="ASIN Frequency (Log10 Adjusted)")
plt.show()

#reviews.rating / ASINs
plt.figure(figsize=(8, 6))
asins_count_ix = pandas_df["asins"].value_counts().index
plt.subplots(2,1,figsize=(16,12))
plt.subplot(2,1,1)
pandas_df["asins"].value_counts().plot(kind="bar", title="ASIN Frequency")
plt.subplot(2,1,2)
sns.pointplot(x="asins", y="reviews.rating", order=asins_count_ix, data=pandas_df)
plt.xticks(rotation=90)
plt.show()

#reviews.doRecommend / ASINs
plt.subplots (2,1,figsize=(16,12))
plt.subplot(2,1,1)
pandas_df["asins"].value_counts().plot(kind="bar", title="ASIN Frequency")
plt.subplot(2,1,2)
sns.pointplot(x="asins", y="reviews.doRecommend", order=asins_count_ix, data=pandas_df)
plt.xticks(rotation=90)
plt.show()

# Convertir les étiquettes de texte en valeurs numériques
df = df.withColumn(
    "label",
    when(col("Sentiment") == "Positive", 1)
    .when(col("Sentiment") == "Negative", 0)
    .when(col("Sentiment") == "Neutral", 0)  # Vous pouvez aussi utiliser 0 pour  une classification binaire
    .otherwise(None)  # Valeurs manquantes
)

# Vérifier les modifications
df.select("Sentiment", "label").show(10)

# Définir la fonction pour transformer le texte en minuscules
def lower_text(line):
    if line is not None:
        return line.lower()
    else:
        return None
# Créer une UDF à partir de la fonction
udflower_text = udf(lower_text, StringType())

# Vérifier les colonnes disponibles dans le DataFrame
df.columns
#Lemmatization
def lemmatize_text(text):
    if text is None:
        return ""
    lmtzr = WordNetLemmatizer()
    words = text.split()
    tagged_words = pos_tag(words)
    lemmatized = []
    for word, tag in tagged_words:
        pos = 'v' if tag.lower().startswith('v') else 'n'
        lemma = lmtzr.lemmatize(word, pos)
        lemmatized.append(lemma)
    return ' '.join(lemmatized)

lemmatize_udf = udf(lemmatize_text, StringType())


# Appliquer la transformation avec la colonne correcte
df = df.withColumn("text_lower", udflower_text(df["`reviews.text`"]))
# Remplacer les valeurs nulles par une chaîne vide dans la colonne 'text_lower'
df = df.withColumn("text_lower", when(col("text_lower").isNull(), "").otherwise(col("text_lower")))
df = df.withColumn("lemmatized_text", lemmatize_udf(col("text_lower")))


# Vérifier le résultat
messages_lower = df.select('text_lower','lemmatized_text', 'label')
messages_lower.show(10)

# Appliquer la tokenisation sur la colonne 'text_lower'
tokenizer = Tokenizer(inputCol="text_lower", outputCol="words")
wordsDataFrame = tokenizer.transform(messages_lower)

# Afficher les premiers exemples après la tokenisation
for words_label in wordsDataFrame.select("words", "label").take(10):
    print(words_label)

# Removing stopwords
remover = StopWordsRemover(inputCol="words", outputCol="words_filtered")
wordsDataFrame1 = remover.transform(wordsDataFrame).select("label","words_filtered")

#Displaying the resultant dataframe
wordsDataFrame1.show(10)

# Stemming the text
stemmer = PorterStemmer()
def stem_tokens(tokens):
    return [stemmer.stem(item) for item in tokens]

def stem_text(tokens):
    stems = stem_tokens(tokens)
    return ' '.join(stems)

udfstem_text=udf(stem_text, StringType())
wordsDataFrame2 = wordsDataFrame1.withColumn("final_text", udfstem_text("words_filtered")).select('final_text','label')

# Caching the RDD
wordsDataFrame2.cache()
#Renaming features for modeling
training = wordsDataFrame2.selectExpr("final_text as text", "label as label")
training = training.withColumn("label", training["label"].cast(DoubleType()))
training.take(5)

"""# TF - IDF Pilpeling"""

#Creating pipeline for Tokenizing, TF - IDF and Logistic Regression Model
tokenizer = Tokenizer(inputCol="text", outputCol="words")
hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol="hashing")
idf = IDF(inputCol=hashingTF.getOutputCol(), outputCol="features")
lr = LogisticRegression(maxIter=10, regParam=0.01)
pipeline = Pipeline(stages=[tokenizer, hashingTF, idf, lr])
# Training the model
model = pipeline.fit(training)

#Predicing Output
prediction = model.transform(training)

#Printing Schema of Prediction Dataset
prediction.printSchema()

# ÉVALUATION
# ==========================
evaluator = BinaryClassificationEvaluator(rawPredictionCol="rawPrediction", labelCol="label")
auc = evaluator.evaluate(prediction, {evaluator.metricName: "areaUnderROC"})
print(f"AUC: {auc:.3f}")

#Printing Error Rate on Training set :-
print ("Error rate is: {0}".format(prediction.rdd.map(lambda line: abs(line[1] - line[7])).reduce(lambda x,y:x+y) / float(prediction.count())))

"""# Word Cloud"""

wordclouddata = wordsDataFrame2.rdd.map(lambda x: (x[1],x[0])).flatMapValues(lambda x: x.split(' ')).flatMapValues(lambda x: x.split('.')).toDF()
wordclouddata = wordclouddata.selectExpr("_1 as Label","_2 as word")
wordclouddata.registerTempTable("words")

from pyspark.sql import SQLContext

# Create an SQLContext instance
sqlContext = SQLContext(my_spark)

# Now run your query
pos_words = sqlContext.sql("SELECT word, round(count(Label) / 100) as cnt FROM words WHERE Label = 1 GROUP BY word ORDER BY cnt DESC").take(1000)
x = ""
for i in pos_words:
        if (i[0] == "<br") | (i[0] == "/><br"):
            pass
        else:
            for a in range(int(i[1])):
                if i[0] == "":
                    pass
                else:
                    x = x +" " +  i[0]

plt.rcParams['figure.figsize'] = (20.0, 10.0)
from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt
wordcloud = WordCloud(stopwords=STOPWORDS,
                          background_color='white',
                          width=1200,
                          height=1000
                         ).generate(x)


plt.imshow(wordcloud)
plt.axis('off')
plt.show()

neg_words = sqlContext.sql(" SELECT word,round(count(Label) / 100) as cnt from words where Label = 0 group by word order by cnt desc").take(1000)

x = ""
for i in neg_words:
        if (i[0] == "<br") | (i[0] == "/><br"):
            pass
        else:
            for a in range(int(i[1])):
                if i[0] == "":
                    pass
                else:
                    x = x +" " +  i[0]

plt.rcParams['figure.figsize'] = (20.0, 10.0)
from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt
wordcloud = WordCloud(stopwords=STOPWORDS,
                          background_color='white',
                          width=1200,
                          height=1000
                         ).generate(x)


plt.imshow(wordcloud)
plt.axis('off')
plt.show()

"""# Bi gram"""

wordsDataFrame_1 = wordsDataFrame1.filter(wordsDataFrame1.label == 1)
wordsDataFrame_0 = wordsDataFrame1.filter(wordsDataFrame1.label == 0)

wordsDataFrame_1.take(2)

ngram = NGram(n=2, inputCol="words_filtered", outputCol="nGrams")
ngram.transform(wordsDataFrame_1).head()

bigrammed = ngram.transform(wordsDataFrame_1)

bigrams = bigrammed.rdd.map(lambda line:line[2])
bigrams.take(2)

all_bigrams = bigrams.flatMap(lambda list:[x for x in list])
top1000_bigrams_1 = all_bigrams.map(lambda x:(x,1)).reduceByKey(lambda x,y:x+y).takeOrdered(1000, key=lambda x: -x[1])
bigrammed_0 = ngram.transform(wordsDataFrame_0)
bigrams_0 = bigrammed_0.rdd.map(lambda line:line[2])
all_bigrams_0 = bigrams_0.flatMap(lambda list:[x for x in list])
top1000_bigrams_0 = all_bigrams_0.map(lambda x:(x,1)).reduceByKey(lambda x,y:x+y).takeOrdered(1000, key=lambda x: -x[1])

sc = my_spark.sparkContext
x1 = sc.parallelize(top1000_bigrams_1)

x2 = x1.map(lambda x:(x[0].replace(' ','_'), x[1]))
wordcloud_1 = x2.toDF()
wordcloud_1 = wordcloud_1.selectExpr("_1 as BiGram","_2 as freq")
wordcloud_1.registerTempTable("positive")


pos_words = sqlContext.sql(" SELECT BiGram, round(freq)/100 as cnt from positive where BiGram NOT LIKE '%/><br%' OR BiGram NOT LIKE '_%' OR BiGram NOT LIKE '%_' order by cnt desc")
pos_words1 = pos_words.collect()
x = ""
for i in pos_words1:
    if i[0][0] == "_":
        pass
    elif i[0][len(i[0])-1] == "_":
        pass
    else:
        for a in range(int(i[1])):
            x = x + " " +  str(i[0])

plt.rcParams['figure.figsize'] = (20.0, 10.0)
from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt

wordcloud = WordCloud(stopwords=STOPWORDS,
                          background_color='white',
                          width=1200,
                          height=1000
                         ).generate(x)
plt.imshow(wordcloud)
plt.axis('off')
plt.show()

x1 = sc.parallelize(top1000_bigrams_0)
x2 = x1.map(lambda x:(x[0].replace(' ','_'), x[1]))
wordcloud_0 = x2.toDF()
wordcloud_0 = wordcloud_0.selectExpr("_1 as BiGram","_2 as freq")
wordcloud_0.registerTempTable("negative")


neg_words = sqlContext.sql(" SELECT BiGram, round(freq)/100 as cnt from negative where BiGram NOT LIKE '%/><br%' OR BiGram NOT LIKE '_%' OR BiGram NOT LIKE '%_' order by cnt desc")
neg_words0 = neg_words.collect()
y = ""
for i in neg_words0:
    if i[0][0] == "_":
        pass
    elif i[0][len(i[0])-1] == "_":
        pass
    else:
        for a in range(int(i[1])):
            y = y + " " +  str(i[0])

plt.rcParams['figure.figsize'] = (20.0, 10.0)
from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt

wordcloud = WordCloud(stopwords=STOPWORDS,
                          background_color='white',
                          width=1200,
                          height=1000
                         ).generate(y)
plt.imshow(wordcloud)
plt.axis('off')
plt.show()

# Texte prétraité depuis le dataframe pandas_df
texts = pandas_df['reviews.text'].fillna("").values.astype("U")

# Vectorisation TF et TF-IDF
cv = CountVectorizer(max_df=0.9, min_df=5, stop_words='english')
tf_m = cv.fit(texts)
tf_d = tf_m.transform(texts)

idf_m = TfidfVectorizer(max_df=0.9, min_df=5, stop_words='english')
tfidf_m = idf_m.fit(texts)
tfidf_d = tfidf_m.transform(texts)

# LDA et KMeans
n_topics = 10

def get_lda(data, topics):
    m = LatentDirichletAllocation(n_components=topics, n_jobs=-1, learning_method='online').fit(data)
    d = m.transform(data)
    return m, d

def get_kmeans(data, k, scale=True):
    if scale:
        s = MinMaxScaler()
        data = s.fit_transform(data)
    m = KMeans(n_clusters=k, n_init=10).fit(data)
    d = m.predict(data)
    return m, d

lda_m, lda_d = get_lda(tf_d, n_topics)
kmean_m, kmean_d = get_kmeans(tfidf_d, n_topics, scale=False)

# Affichage des topics

def show_topics(model, feature_names, n_words):
    for topic_idx, topic in enumerate(model.components_):
        print(f"Topic #{topic_idx}:")
        print(", ".join([feature_names[i] for i in topic.argsort()[:-n_words - 1:-1]]))
    print()

print("Top 15 stemmed words per topic in LDA model\n")
show_topics(lda_m, tf_m.get_feature_names_out(), 15)

# Affichage des mots par cluster (KMeans)

def show_cluster_topics(cluster_labels, tf_matrix, feature_names, n_words):
    d = pd.DataFrame(tf_matrix.toarray())
    d['c'] = cluster_labels
    d = d.groupby('c').sum().T

    for col in d:
        top_n = d[col].nlargest(n_words).index.tolist()
        print(f"Cluster #{col}:")
        print(", ".join([feature_names[i] for i in top_n]))
    print()

print("Top 15 stemmed words per cluster in Kmeans model\n")
show_cluster_topics(kmean_d, tfidf_d, tfidf_m.get_feature_names_out(), 15)

# Réduction dimensionnelle et t-SNE

def get_svd(data, components):
    svd = TruncatedSVD(n_components=components).fit(data)
    o = pd.DataFrame(svd.transform(data), columns=range(0, components))
    return svd, o

def get_tsne(data, components, perplexity):
    tsne = TSNE(n_components=components, perplexity=perplexity, n_iter=1000)
    o = pd.DataFrame(tsne.fit_transform(data), columns=range(0, components))
    return tsne, o

svd_v, svd_m = get_svd(tfidf_d, 50)
tnse_v, tsne_m = get_tsne(svd_m, 2, 25)

lda_c = lda_d.argmax(axis=1)

# Fonction de plot

def plot_scatter_2d(x, y, c, sample_size, title):
    df = pd.DataFrame({'x': x, 'y': y, 'c': c}).sample(sample_size)
    l = len(np.unique(c))
    ax = plt.subplot(111)
    colors = cm.rainbow(np.linspace(0, 1, l))

    for c in range(0, l):
        qq = df[df['c'] == c]
        ax.scatter(qq['x'], qq['y'], c=[colors[c]], label=c)

    plt.legend(loc='upper left', numpoints=1, ncol=3, fontsize=8, bbox_to_anchor=(0, 0), title='Topic/Cluster')
    ax.set_yticklabels([])
    ax.set_xticklabels([])
    ax.set_title(title)
    plt.show()

plot_scatter_2d(tsne_m[0], tsne_m[1], kmean_d, 1000, 'KMeans Clustering of Reviews using TFIDF + t-SNE')
plot_scatter_2d(tsne_m[0], tsne_m[1], lda_c, 1000, 'LDA Topics of Reviews using CountVectorizer + t-SNE')



























